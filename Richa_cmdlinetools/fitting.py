{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys, os, argparse\n",
    "import ase.io as ase_io\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import math\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Command-line utility to compute NICE features. Training and testing is done here. We need the following from the user:\n",
    "    1. The database file. \n",
    "    2. The name for final output file.\n",
    "    2. Index for training commands for ase.io.read commands.\n",
    "    3. number of environments to fit nice transfomers\n",
    "    4. I will define the grid. \n",
    "    5. Input for HYPERS parameters required from user. Keeping 'gaussian_sigma_type': 'Constant','cutoff_smooth_width': 0.3, and 'radial_basis': 'GTO'\n",
    "    6. Input for standardblocks- \n",
    "        Covariants: num_expand: Number of the most important input pairs to be considered for expansion.\n",
    "                    max_take: Number of features to be considered for purification step. The default value is None.\n",
    "                    n_components: Number of components for the PCA step. Default value None takes number of components equal to the number of covariants for each individual lambda.\n",
    "        Invariants: num_expand: Number of the most important input pairs to be considered for expansion.\n",
    "                    max_take: Number of features to be considered for purification step. The default value is None.\n",
    "                    n_components: Number of components for the PCA step. Default value None takes number of components equal to the number of covariants for each individual lambda.\n",
    "                    \n",
    "    \"\"\"\n",
    "    #Tweak the autogenerated help output to look nicer (Keeping it same from previous file.)\n",
    "        \n",
    "    formatter = lambda prog: argparse.HelpFormatter(prog, max_help_position=22)\n",
    "    #parser = argparse.ArgumentParser(description=main.__doc__, formatter_class=formatter)\n",
    "    parser = argparse.ArgumentParser(description='Trial- to check where the main.__doc__ is', formatter_class=formatter)\n",
    "    parser.add_argument('input', type=str, default=\"\", nargs=\"?\", help='XYZ file to load')\n",
    "    parser.add_argument('-o', '--output', type=str, default=\"\", help='Output files prefix. Defaults to input filename with stripped extension')\n",
    "    parser.add_argument('-w','--which_output', type=int, default=1, help='1 for getting a different NICE for each species or else a single NICE for all species')\n",
    "    parser.add_argument('--train_subset', type=str, default=\"0:10000\", help='Index for reading the file for training in ASE format')\n",
    "    #parser.add_argument('--test_subset', type=str, default=\"10000:15000\", help='Index for reading the file for testing in ASE format')\n",
    "    parser.add_argument('--environments_for_fitting', type=int, default=1000, help='Number of environments for fitting')\n",
    "    parser.add_argument('--interaction_cutoff', type=int, default=6.3, help='Interaction cut-off')\n",
    "    parser.add_argument('--max_radial', type=int, default=5, help='Number of radial channels')\n",
    "    parser.add_argument('--max_angular', type=int, default=5, help='Number of angular momentum channels')\n",
    "    parser.add_argument('--gaussian_sigma_constant', type=int, default=6.3, help='Gaussian smearing')\n",
    "    parser.add_argument('--numexpcov', type=int, default=150, help='Number of the most important input pairs to be considered for expansion.')\n",
    "    parser.add_argument('--numexpcinv', type=int, default=300, help='Number of the most important input pairs to be considered for expansion.')\n",
    "    parser.add_argument('--maxtakecov', type=int, default=None, help='Number of features to be considered for purification step.')\n",
    "    parser.add_argument('--maxtakeinv', type=int, default=None, help='Number of features to be considered for purification step.')\n",
    "    parser.add_argument('--ncompcov', type=int, default=None, help='Number of components for the PCA step.')\n",
    "    parser.add_argument('--ncompinv', type=int, default=None, help='Number of components for the PCA step.')\n",
    "    parser.add_argument('--json', type=str, default='{}', help='Additional hypers, as JSON string')\n",
    "    \n",
    "    \n",
    "    #parser.add_argument('--select', type=str, default=\":\", help='Selection of input frames. ASE format.')\n",
    "    #parser.add_argument('--nice', type=str, default=\"nice.pickle\", help='Definition of the NICE contraction. Output from optimize_nice.py')\n",
    "    #parser.add_argument('--blocks', type=int, default=1, help='Number of blocks to break the calculation into.')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #File inputs\n",
    "    filename = args.input\n",
    "    output   = args.output\n",
    "    whichoutput = args.which_output\n",
    "    #select   = args.select    \n",
    "    #nice     = args.nice\n",
    "    #nblocks  = args.blocks\n",
    "    \n",
    "        \n",
    "    #filename = 'methane.extxyz'\n",
    "    #output = 'out'\n",
    "    #select = ':'    \n",
    "    #nice = 'nice.pickle'\n",
    "    #nblocks = 1\n",
    "    \n",
    "    #ASE read inputs\n",
    "    train_subset = args.train_subset\n",
    "    train_subset_num = [int(s) for s in re.findall(r'\\b\\d+\\b', train_subset)][1]\n",
    "    #test_subset = args.test_subset\n",
    "    environments_for_fitting = args.environments_for_fitting\n",
    "    \n",
    "    #Hyper inputs\n",
    "    ic = args.interaction_cutoff\n",
    "    n = args.max_radial\n",
    "    l = args.max_angular\n",
    "    sig = args.gaussian_sigma_constant\n",
    "    json_hypers = json.loads(args.json)\n",
    "    \n",
    "    #get_NICE inputs\n",
    "    numexcov = args.numexpcov\n",
    "    numexpinv = args.numexpinv\n",
    "    maxtakecov = args.maxtakecov\n",
    "    maxtakeinv = args.maxtakeinv\n",
    "    ncompcov = args.ncompcov\n",
    "    ncompinv = args.ncompinv\n",
    "    \n",
    "    \n",
    "    #Output file\n",
    "    if output == \"\":\n",
    "        output = os.path.splitext(filename)[0]\n",
    "        \n",
    "        \n",
    "    #Some constants    \n",
    "    HARTREE_TO_EV = 27.211386245988\n",
    "    \n",
    "    grid = [] #for learning curve, should there be a pattern in forming this grid\n",
    "    gr1=1;\n",
    "    grid_point=0;\n",
    "    While grid_point<train_subset_num: \n",
    "        grid_point=102.44*math.exp(gr1*0.3837);\n",
    "        grid.append(grid_point)\n",
    "        gr1+=1\n",
    "        \n",
    "    print(\"Using the grid:\",grid)\n",
    "    \n",
    "    #Building HYPERS\n",
    "    HYPERS = { **{\n",
    "    'interaction_cutoff': ic,\n",
    "    'max_radial': n,\n",
    "    'max_angular': l,\n",
    "    'gaussian_sigma_type': 'Constant',\n",
    "    'gaussian_sigma_constant': sig,\n",
    "    'cutoff_smooth_width': 0.3,\n",
    "    'radial_basis': 'GTO'\n",
    "    }, **json_hypers }\n",
    "  \n",
    "    \"\"\"\n",
    "    Definitions of what we seem to be doing!\n",
    "    StandardSequence --> Block implementing logic of main NICE sequence.\n",
    "    StandardBlock --> Block for standard procedure of body order increasement step for covariants and invariants.\n",
    "    a. ThresholdExpansioner --> Covariant [Block to do Clebsch-Gordan iteration. It uses two even-odd pairs of Data instances with covariants\n",
    "    to produce new ones. If first even-odd pair contains covariants of body order v1, and the second v2, body\n",
    "    order of the result would be v1 + v2.]\n",
    "    b. CovariantsPurifierBoth --> Covariant [Block to purify covariants of both parities. It operates with pairs of instances of Data class with covariants]\n",
    "    c. IndividualLambdaPCAsBoth --> Covariant [Block to do pca step for covariants of both parities. It operates with even-odd pairs of instances of Data class]\n",
    "    d. ThresholdExpansioner --> Invariant [Block to do Clebsch-Gordan iteration.]\n",
    "    e. InvariantsPurifier --> Invariant [Block to purify invariants. It operates with numpy 2d arrays containing invariants]\n",
    "    f. InvariantsPCA --> Invariant [Block to do pca step for invariants. It operates with 2d numpy arrays]\n",
    "\n",
    "    \"\"\"\n",
    "    def get_nice():\n",
    "    numax=4;\n",
    "    sb = [ ]\n",
    "    for nu in range(1, numax-1): # this starts from nu=2 actually\n",
    "        sb.append(\n",
    "            StandardBlock(ThresholdExpansioner(num_expand=numexpcov),\n",
    "                      CovariantsPurifierBoth(max_take=maxtakecov),\n",
    "                      IndividualLambdaPCAsBoth(n_components=ncompcov),\n",
    "                      ThresholdExpansioner(num_expand=numexpinv, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=maxtakeinv),\n",
    "                      InvariantsPCA(n_components=ncompinv)) \n",
    "                         )\n",
    "                         \n",
    "    # at the last order, we only need invariants\n",
    "    sb.append(\n",
    "            StandardBlock(None, None, None,\n",
    "                         ThresholdExpansioner(num_expand=numexpinv, mode='invariants'),\n",
    "                         InvariantsPurifier(max_take=maxtakeinv),\n",
    "                         InvariantsPCA(n_components=ncompinv)) \n",
    "                         )\n",
    "    \n",
    "    return StandardSequence(sb,initial_scaler=InitialScaler(mode='signal integral', individually=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Reading the file into training and testing  \n",
    "    print(\"Loading structures for train \", filename, \" frames: \", train_subset)    \n",
    "    train_structures = ase.io.read(filename, index=train_subset)\n",
    "    #print(\"Loading structures for test \", filename, \" frames: \", test_subset)\n",
    "    #test_structures = ase.io.read(filename, index=test_subset)\n",
    "    \n",
    "    all_species = get_all_species(train_structures)\n",
    "    \n",
    "    #calculating the coefficiencts\n",
    "    \"\"\"\n",
    "    [environmental index, radial basis/neighbor specie index, lambda, m] with spherical expansion coefficients for \n",
    "        environments around atoms with specie indicated in key.\n",
    "    \"\"\"\n",
    "    train_coefficients = get_spherical_expansion(train_structures, HYPERS, all_species)\n",
    "    #test_coefficients = get_spherical_expansion(test_structures, HYPERS, all_species)\n",
    "    \n",
    "    if args.which_output:\n",
    "        #individual nice transformers for each atomic specie in the dataset\n",
    "        nice = {}\n",
    "        for key in train_coefficients.keys():\n",
    "            nice[key] = get_nice()\n",
    "    else:\n",
    "        #Now that we have the coefficients, we want to fit a single nice transformer irrespective of the specie\n",
    "        #1. Take all coefficients for each specie and merge all the coefficients together.Then based on the user-defined number of environments to be used for fitting, we choose the required number of coefficients.\n",
    "        all_coefficients = [train_coefficients[key] for key in train_coefficients.keys()]\n",
    "        all_coefficients = np.concatenate(all_coefficients, axis=0)\n",
    "        np.random.shuffle(all_coefficients)\n",
    "        all_coefficients = all_coefficients[0:environments_for_fitting]\n",
    "        #2. Use the model to fit nice on the coefficients chosen above\n",
    "        nice_single = get_nice()\n",
    "        nice_single.fit(all_coefficients)\n",
    "        #3. Irrespective of the central specie, we use the same nice transformer\n",
    "        nice = {specie: nice_single for specie in all_species}\n",
    "    \n",
    "    HYPERS[\"ncompcov\"] = ncompcov\n",
    "    HYPERS[\"ncompinv\"] = ncompinv\n",
    "    HYPERS[\"numexpcov\"] = numexpcov\n",
    "    HYPERS[\"numexpinv\"] = numexpinv               \n",
    "    HYPERS[\"reference-file\"] = filename\n",
    "    HYPERS[\"reference-sel\"] = train_subset\n",
    "    \n",
    "    print(\"Dumping NICE model\")    \n",
    "    pickle.dump( { \n",
    "               \"HYPERS\" : HYPERS, \n",
    "               \"NICE\": nice,\n",
    "             }, open(output+\".pickle\", \"wb\"))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
