{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor or non standard sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials it was discussed how to perform calculations with standard NICE scheme, which is reflected by class StandardSequence. But NICE toolbox provides broader opportunities. It is possible, for example, to combine latest covariants with each other at each step in order to get 2^n body order features after n iterations. \n",
    "\n",
    "In previous tutorials model was defined by StandardSequence class, whose initialization method accepts instances of other classes as ThresholdExpansioner or InvariantsPurifier. These blocks can be used by their own to construct custom model.\n",
    "\n",
    "First of all we need to calculate spherical expansion coefficients as in previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-12 21:26:57--  https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\n",
      "Resolving archive.materialscloud.org (archive.materialscloud.org)... 148.187.96.41\n",
      "Connecting to archive.materialscloud.org (archive.materialscloud.org)|148.187.96.41|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://object.cscs.ch/archive/b6/12/d8e3-58af-4374-96ba-b3551ac5d2f4/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dmethane.extxyz.gz&Expires=1602530877&Signature=VYUS8wL0D0Oadx%2BwOI4W57%2BAO5Q%3D&AWSAccessKeyId=ee64314446074ed3ab5f375a522a4893 [following]\n",
      "--2020-10-12 21:26:57--  https://object.cscs.ch/archive/b6/12/d8e3-58af-4374-96ba-b3551ac5d2f4/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dmethane.extxyz.gz&Expires=1602530877&Signature=VYUS8wL0D0Oadx%2BwOI4W57%2BAO5Q%3D&AWSAccessKeyId=ee64314446074ed3ab5f375a522a4893\n",
      "Resolving object.cscs.ch (object.cscs.ch)... 148.187.25.200, 148.187.25.201, 148.187.25.202\n",
      "Connecting to object.cscs.ch (object.cscs.ch)|148.187.25.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1218139661 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘methane.extxyz.gz’\n",
      "\n",
      "methane.extxyz.gz   100%[===================>]   1.13G  21.8MB/s    in 37s     \n",
      "\n",
      "2020-10-12 21:27:34 (31.5 MB/s) - ‘methane.extxyz.gz’ saved [1218139661/1218139661]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 29.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 162.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# downloading dataset from https://archive.materialscloud.org/record/2020.110\n",
    "\n",
    "!wget \"https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\" -O methane.extxyz.gz\n",
    "!gunzip -k methane.extxyz.gz\n",
    "\n",
    "import numpy as np\n",
    "import ase.io\n",
    "import tqdm\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "structures = ase.io.read('methane.extxyz', \n",
    "                         index = '0:1000')\n",
    "\n",
    "HYPERS = {\n",
    "'interaction_cutoff': 6.3,\n",
    "'max_radial': 5,\n",
    "'max_angular': 5,\n",
    "'gaussian_sigma_type': 'Constant',\n",
    "'gaussian_sigma_constant': 0.05,\n",
    "'cutoff_smooth_width': 0.3,\n",
    "'radial_basis': 'GTO'\n",
    "}\n",
    "\n",
    "all_species = get_all_species(structures)\n",
    "\n",
    "coefficients = get_spherical_expansion(structures, HYPERS, all_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to this point coefficients for each central specie are 4 dimensional numpy array with indexing [environmental index, radial basis/specie index, lambda, m]\n",
    "\n",
    "Let's focus on only H centered environments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10, 6, 11)\n"
     ]
    }
   ],
   "source": [
    "coefficients = coefficients[1]\n",
    "print(coefficients.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to perform initial scaling, as it was discussed in the first tutorial. For this purposes there is class InitialScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_scaler = InitialScaler(mode = 'signal integral', individually = False)\n",
    "initial_scaler.fit(coefficients)\n",
    "coefficients = initial_scaler.transform(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If individually is set to False this class requires fitting before transforming the data. Otherwise fitting is not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to track parity of covariants, i. e. keep even and odd features separated, we need to split them at the begining of our calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nice.nice_utilities.Data'>\n",
      "(4000, 10, 6, 11)\n",
      "even features sizes:  [10, 0, 10, 0, 10, 0]\n",
      "odd features sizes:  [0, 10, 0, 10, 0, 10]\n"
     ]
    }
   ],
   "source": [
    "data_even_1, data_odd_1  = InitialTransformer().transform(coefficients)\n",
    "print(type(data_even_1))\n",
    "print(data_even_1.covariants_.shape)\n",
    "print(\"even features sizes: \", data_even_1.actual_sizes_)\n",
    "print(\"odd features sizes: \", data_odd_1.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is couple of Data instances which was already discussed in the tutorial \"Calculating covariants\".\n",
    "\n",
    "All spherical expansion coefficients with even l remain constant under reflections, i. e. are even covariants, while all spherical expansion coefficients with odd l changes sign under reflection, i. e. are odd covariants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and purifiers blocks has two versions. One to transform single instance of data of certain parity, and the second is for the same transformation of both. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 5 0 5 0]\n"
     ]
    }
   ],
   "source": [
    "pca = IndividualLambdaPCAs(n_components = 5) #single parity version\n",
    "pca.fit(data_even_1)\n",
    "data_even_1_t = pca.transform(data_even_1)\n",
    "print(data_even_1_t.actual_sizes_)\n",
    "\n",
    "pca = IndividualLambdaPCAsBoth() #both version\n",
    "pca.fit(data_even_1, data_odd_1)\n",
    "data_even_1_t, data_odd_1_t = pca.transform(data_even_1, data_odd_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common thing among PCA and purifiers blocks is num_to_fit semantics. Each class has num_to_fit argument in the initialization, which by default equals to '10x'. If num_to_fit is string of 'number x' format it would cause corresponding class use no more than number multiplier by number of components in case of pca, or number multiplier by number of coefficients in linear regression in case of purifiers data points. Data points are calculated as all entries of covariants. I. e. for lambda = 3 for example each environment would bring (3 * 2 + 1) data points, since dimensionality of single covariant vector is (2 * lambda + 1). If num_to_fit is int, it would do the same using the provided number as the upper bound for number of datapoints not depending on the actual number of pca components or linear regression coefficients. If total available number of data points is less than the number specified by num_to_fit class would raise warning, that there are not enough data. \n",
    "\n",
    "This is done because the overall model is very diverse, and different parts of the model requires very different amount of data for good fitting. Thus, it is a good idea to do such restrictions to speed up the process. \n",
    "\n",
    "In case of PCA if n_components specified in the constructor is less than the actual number of features given during the fit step, it would be decreased to actual number of features.\n",
    "But, if number of data points is less than number of components after this possible decreasement (which make it impossible to produce such amount of components) it would raise ValueError with demand to provide more data for fitting. \n",
    "\n",
    "In order to do PCA step in invariants branch there is class InvariantsPCA, which actually differs from sklearn.decomposition.PCA only by num_to_fit semantics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:218: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 10, desired number of environments is 3000, actual number of environments is 400.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n"
     ]
    }
   ],
   "source": [
    "pca = InvariantsPCA(num_to_fit = '300x')\n",
    "ar  = np.random.rand(400, 10)\n",
    "pca.fit(ar)\n",
    "print(pca.transform(ar).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purifiers there are classes CovariantsPurifier, CovariantsPurifierBoth, \n",
    "InvariantsPurifier, and CovariantsIndividualPurifier. Their purpose is to transform data of single parity, both chunks of data, invariants, and single lambda channel respectively.\n",
    "\n",
    "Their fit and transform methods accept list of covariants/invariants of previous body orders along with current body order. For example: (Let's pretend that we have already features of several body orders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "purifier = CovariantsPurifier(max_take = 3)\n",
    "purifier.fit([data_even_1, data_even_1], data_even_1)\n",
    "data_even_1_t = purifier.transform([data_even_1, data_even_1], data_even_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was already mentioned in the first tutorial purifiers can accept arbitrarily sklearn shaped linear regressors, i. e. with fit and predict methods. See tutorial \"Custom regressors into purifiers\" for example of such custom regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do expansion with thresholding euristics it is necessary to get information how important are particular features. One way is to assing .importance_ property in the Data class (setter will be done in the next version of NICE). The other is to pass features through pca, which would automatically asign importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IndividualLambdaPCAsBoth() \n",
    "pca.fit(data_even_1, data_odd_1)\n",
    "data_even_1, data_odd_1 = pca.transform(data_even_1, data_odd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThresholdExpansioner's fit and transform methods accept two even-odd pair of datas. If first pair is of body order v1 and second pair is of body order v2, result would be of body order v1 + v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 70  69 165 142 176 121]\n",
      "[  0 124 112 178 140 150]\n"
     ]
    }
   ],
   "source": [
    "expansioner = ThresholdExpansioner(num_expand = 200)\n",
    "\n",
    "expansioner.fit(data_even_1, data_odd_1, data_even_1, data_odd_1)\n",
    "data_even_2, data_odd_2 = expansioner.transform(data_even_1, data_odd_1,\\\n",
    "                                                data_even_1, data_odd_1)\n",
    "print(data_even_2.actual_sizes_)\n",
    "print(data_odd_2.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most time during the fitting is consumed for precomputing clebsch-gordan coefficients. Thus, in case of frequent expansioners fitting with same lambda_max, it is a good idea to precompute clebsch-gordan coefficients once, and after that just feed expansioners with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clebsch = nice.clebsch_gordan.ClebschGordan(5) # 5 is lamba max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go to 1024 body order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_even_now, data_odd_now = data_even_1, data_odd_1\n",
    "\n",
    "\n",
    "for _ in tqdm.tqdm(range(10)):        \n",
    "    pca = IndividualLambdaPCAsBoth(10)\n",
    "    pca.fit(data_even_now, data_odd_now)\n",
    "    data_even_now, data_odd_now  = pca.transform(data_even_now, data_odd_now)\n",
    "    expansioner = ThresholdExpansioner(50)\n",
    "    expansioner.fit(data_even_now, data_odd_now, data_even_now, data_odd_now, clebsch_gordan = clebsch)\n",
    "    data_even_now, data_odd_now = expansioner.transform(data_even_now, data_odd_now, data_even_now, data_odd_now)\n",
    "\n",
    "    # very high body order cause numerical instabilities,\n",
    "    # and, thus, there is need to normalize data   \n",
    "    for lambd in range(6):\n",
    "        if (data_even_now.actual_sizes_[lambd] > 0):\n",
    "            even_factor = np.sqrt(np.mean(data_even_now.covariants_[:, :data_even_now.actual_sizes_[lambd], lambd] ** 2))\n",
    "            if (even_factor > 1e-15): #catch exact zeros\n",
    "                data_even_now.covariants_[:, :data_even_now.actual_sizes_[lambd], lambd] /= even_factor\n",
    "\n",
    "        if (data_odd_now.actual_sizes_[lambd] > 0):\n",
    "            odd_factor = np.sqrt(np.mean(data_odd_now.covariants_[:, :data_odd_now.actual_sizes_[lambd], lambd] ** 2))\n",
    "            if (odd_factor > 1e-15): #catch exact zeros\n",
    "                data_odd_now.covariants_[:, :data_odd_now.actual_sizes_[lambd], lambd] /= odd_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 28, 6, 11)\n",
      "[ 7 19 25 28 28 25]\n",
      "[ 8 18 24 26 28 26]\n"
     ]
    }
   ],
   "source": [
    "print(data_even_now.covariants_.shape)\n",
    "print(data_even_now.actual_sizes_)\n",
    "print(data_odd_now.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
