{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing machine learning potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to get some dataset for fitting. Good example\n",
    "is [this one ](https://archive.materialscloud.org/record/2020.110):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-11 22:55:57--  https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\n",
      "Resolving archive.materialscloud.org (archive.materialscloud.org)... 148.187.96.41\n",
      "Connecting to archive.materialscloud.org (archive.materialscloud.org)|148.187.96.41|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://object.cscs.ch/archive/b6/12/d8e3-58af-4374-96ba-b3551ac5d2f4/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dmethane.extxyz.gz&Expires=1602449817&Signature=ipI5BxoFpZfU08isT3SnYOzzOzg%3D&AWSAccessKeyId=ee64314446074ed3ab5f375a522a4893 [following]\n",
      "--2020-10-11 22:55:57--  https://object.cscs.ch/archive/b6/12/d8e3-58af-4374-96ba-b3551ac5d2f4/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dmethane.extxyz.gz&Expires=1602449817&Signature=ipI5BxoFpZfU08isT3SnYOzzOzg%3D&AWSAccessKeyId=ee64314446074ed3ab5f375a522a4893\n",
      "Resolving object.cscs.ch (object.cscs.ch)... 148.187.25.200, 148.187.25.201, 148.187.25.202\n",
      "Connecting to object.cscs.ch (object.cscs.ch)|148.187.25.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1218139661 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘methane.extxyz.gz’\n",
      "\n",
      "methane.extxyz.gz   100%[===================>]   1.13G  54.2MB/s    in 26s     \n",
      "\n",
      "2020-10-11 22:56:23 (44.3 MB/s) - ‘methane.extxyz.gz’ saved [1218139661/1218139661]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downloading dataset from https://archive.materialscloud.org/record/2020.110\n",
    "\n",
    "!wget \"https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\" -O methane.extxyz.gz\n",
    "!gunzip -k methane.extxyz.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ase.io\n",
    "import tqdm\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell parameters which control subsequent calculations along with hyperparamters of the model are defined. \n",
    "\n",
    "\n",
    "Total amount of structures in the methane dataset is very huge, thus it is good idea to select smaller amount to speed up calculations. \n",
    "\n",
    "\n",
    "Two out of three steps of NICE requires data to be fitted. In the PCA step atomic environments are used to determine matrix of linear transformation which would allow to \n",
    "preserve the most amount of information for **this particular dataset**. In purifers eliminated correlations are also dataset specific. Though, it is absolutelly not necessary \n",
    "to use the same amount of data to fit NICE transformer and to fit subsequent machine learning model. Typically NICE transformer requires less amount of data to be fitted, and\n",
    "fitting process requires noticiable amount of RAM, thus, it is good idea to restrict amount of data for this step, which is controleed by environments_for_fitting variable. \n",
    "\n",
    "grid defines numbers of training configurations for which error would be estimated in order to get an idea how good is the model depending on the number of training configurations. \n",
    "(yep, NICE transformer uses more data for fitting for few first point, but it is just a tutorial)\n",
    "\n",
    "in HYPERS dictionary parameters for initial spherical expansion are defined. For more detail we refer reader to [librascal](https://github.com/cosmo-epfl/librascal) documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HARTREE_TO_EV = 27.211386245988\n",
    "train_subset = \"0:10000\"    #input for ase.io.read command\n",
    "test_subset = \"10000:15000\"     #input to ase.io.read command\n",
    "environments_for_fitting = 1000    #number of environments to fit nice transfomers\n",
    "grid =  [150, 200, 350, 500, 750, 1000,\n",
    "         1500, 2000, 3000, 5000, 7500, 10000] #for learning curve\n",
    "\n",
    "#HYPERS for librascal spherical expansion coefficients\n",
    "HYPERS = {\n",
    "'interaction_cutoff': 6.3,\n",
    "'max_radial': 5,\n",
    "'max_angular': 5,\n",
    "'gaussian_sigma_type': 'Constant',\n",
    "'gaussian_sigma_constant': 0.05,\n",
    "'cutoff_smooth_width': 0.3,\n",
    "'radial_basis': 'GTO'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the most important one. Our model is defined here. As was mentioned before \n",
    "NICE is the sequence of standard transformations, there each increases body order by 1. \n",
    "The classes which implements this logic are StandardBlock and StandardSequence. \n",
    "\n",
    "StandardSequence consist of 1) initial scaler 2) initial pca and of 3) sequence of standard blocks. \n",
    "\n",
    "Let's imagine uniform multiplication of spherical expansion coefficients by some constant k. In this case covariants of order k would change as *= k ^(body order). In other words relative scale of different body orders would change. This might affect subsequent regression, so it is a good idea to fix the scale in some proper way. This is done by initial scaler. It has two mods - \"signal integral\" and \"variance\". In the first case it scales coefficients such a way to make integral of squared corresponding signal over the ball to be one. In the second case it assures variance of coefficient's entries to be ones. In practice first mod gives better results. Second parameter to this class is to scale coefficients individually, i. e. separately for each environment or globally, thus preserving information about scale of signals in relation to each other. \n",
    "\n",
    "Initial pca is the same pca which is applied in each block, more details about it later. It is the first transformation to coefficients after initial scaler. \n",
    "\n",
    "As already mentioned in theory each block consist of two branches - for covariants and for invariants. Each branch consist of expansion, purification and pca step. During the expansion step in each block features of next body order are produced by Clebsch-Gordan iteration between features from the previous block and spherical expansion coefficients after initial_pca. In case of full expansion, each with each, number of features after this transformation would be incredibly huge, as it was already discussed in theory. Thus, thresholding heuristic is used. For each feature information how important it is is stored during the calculations. In standard sequence this importances are just explained variance ratios after PCA step. During expansion for each pair of covariant vectors \"pair importance\" is defined as the multiplication of previously discussed single feature importances, and after that only fixed amount of most important input pairs produce output. This fixed amount is controlled by num_expand parameter. If it is not specified (or set to None) full expansion, each with each, would be performed. \n",
    "\n",
    "The nature of purifier step was discussed in the theory. Parameter max_take controls the amount of features to take for purification from previous body orders. (Features are alwas stored in descending order of importance, and it uses first ones). If max_take is not specified (None) it would use all available features. \n",
    "One additional parameter is linear regressor to use. For example \n",
    "\n",
    "from sklearn.linear_model import Ridge<br>\n",
    "CovariantsPurifierBoth(regressor = Ridge(alpha = 42, fit_intercept = False), max_take = 10)\n",
    "\n",
    "or \n",
    "\n",
    "from sklearn.linear_model import Lars<br>\n",
    "InvariantsPurifier(regressor = Lars(n_nonzero_coefs = 7), max_take = 10)\n",
    "\n",
    "Default one is Ridge(alpha = 1e-12) without fitting intercept for covariants purifier and with fitting intercept for invariants purifier. \n",
    "\n",
    "***Important!*** always put fit_intercept = False to regressor in covariants purifer. Otherwise resulting features would not be covariants, since vector with constant entries is not covariant. (it is not checked automatically, since corresponding parameter might have different name from \"fit_intercept\")\n",
    "\n",
    "Custom regressors can be feeded into purifiers, more details about it in tutorial \"Custom regressors into purifiers\"\n",
    "\n",
    "Parameter of pca states for the number of output features. If it is not specified (None) full pca would be performed. \n",
    "\n",
    "Both in name of classes states for the fact that transformations are done simultaneously on even and odd features (more details about it in the tutorials \"Calculating covariants\" (what are even and odd features?) and \"Constructor or non standard sequence\" (classes to work with no separation?)). \n",
    "\n",
    "Individual in IndividualLambdaPCAsBoth states for the fact that transformations are independent for each lambda channel.\n",
    "\n",
    "Since we are interested only in invariants, it is not necessary for last block to calculate covariants. Thus corresponding branch is filled with None-s.\n",
    "\n",
    "In this example parameters of covariant and invariant branches (such as num_expand in expansioners) are not dramatically different, but in real life calculations they usually differ from each other dramatically, see examples folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our model:\n",
    "def get_nice():\n",
    "    return StandardSequence([StandardBlock(ThresholdExpansioner(num_expand = 150),\n",
    "                                              CovariantsPurifierBoth(max_take = 10),\n",
    "                                                  IndividualLambdaPCAsBoth(n_components = 50),\n",
    "                                                 ThresholdExpansioner(num_expand =300, mode = 'invariants'),\n",
    "                                                 InvariantsPurifier(max_take = 50),\n",
    "                                                  InvariantsPCA(n_components = 200)),\n",
    "                             StandardBlock(ThresholdExpansioner(num_expand = 150),\n",
    "                                              CovariantsPurifierBoth(max_take = 10),\n",
    "                                                  IndividualLambdaPCAsBoth(n_components = 50),\n",
    "                                                 ThresholdExpansioner(num_expand =300, mode = 'invariants'),\n",
    "                                                 InvariantsPurifier(max_take = 50),\n",
    "                                                  InvariantsPCA(n_components = 200)),\n",
    "                            StandardBlock(None,\n",
    "                                             None,\n",
    "                                                  None,\n",
    "                                                  ThresholdExpansioner(num_expand =300, mode = 'invariants'),\n",
    "                                              InvariantsPurifier(max_take = 50),\n",
    "                                                 InvariantsPCA(n_components = 200))\n",
    "                                   ],\n",
    "                            initial_scaler = InitialScaler(mode = 'signal integral',\n",
    "                                                           individually = True)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to always fill all transformation steps. For example the following block is valid:\n",
    "\n",
    "<pre>\n",
    "StandardBlock(ThresholdExpansioner(num_expand = 150),\n",
    "              None, \n",
    "              IndividualLambdaPCAsBoth(n_components = 50), \n",
    "              ThresholdExpansioner(num_expand =300, mode = 'invariants'), \n",
    "              InvariantsPurifier(max_take = 50), \n",
    "              None)\n",
    "</pre>   \n",
    "  \n",
    "In this case purifying step in covariants branch and PCA step in invariants branch would be ommited. Covariants and invariants branches are independent. In case of not valid combinations, such as \n",
    "\n",
    "<pre>\n",
    "StandardBlock(None, \n",
    "              None,\n",
    "              IndividualLambdaPCAsBoth(n_components = 50), \n",
    "              ...)\n",
    "</pre>              \n",
    "\n",
    "It would raise value error with the description of the problem during initialization.\n",
    "\n",
    "All intermediate blocks must compute covariants. Block is considered as computing covariants if it contains covariants expansioner and covariants pca. The latter is required since expansioners in subsequent blocks require not only covariants themselves, but also their importances for thresholdoing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we read structures, get set of all species in the dataset, and calculate spherical expansion. \n",
    "\n",
    "all species is given by the set of int, where 1 states for H, 2 for He and so on. \n",
    "\n",
    "coefficients is the dictionary where keys are central species, 1 and 6 in our case, and entries are numpy arrays shaped in the  [environment_index, radial/specie index, l, m] way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all species:  [1 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 30.58it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 51.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 74.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train_structures = ase.io.read('methane.extxyz', \n",
    "                         index = train_subset)\n",
    "\n",
    "test_structures = ase.io.read('methane.extxyz', \n",
    "                         index = test_subset)\n",
    "\n",
    "all_species = get_all_species(train_structures + test_structures)\n",
    "print(\"all species: \", all_species)\n",
    "train_coefficients = get_spherical_expansion(train_structures, HYPERS, all_species)\n",
    "\n",
    "\n",
    "test_coefficients = get_spherical_expansion(test_structures, HYPERS, all_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fit two NICE transformers on environments around H and C atoms separatelly.\n",
    "the following cells create them and perfom fitting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#individual nice transformers for each atomic specie in the dataset\n",
    "nice = {}\n",
    "for key in train_coefficients.keys():\n",
    "    nice[key] = get_nice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:201: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 200, desired number of environments is 2000, actual number of environments is 1000.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n",
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:201: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 200, desired number of environments is 2000, actual number of environments is 1000.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n",
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:201: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 200, desired number of environments is 2000, actual number of environments is 1000.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n",
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:201: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 200, desired number of environments is 2000, actual number of environments is 1000.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n",
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:201: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 200, desired number of environments is 2000, actual number of environments is 1000.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n",
      "/home/pozdn/.local/lib/python3.6/site-packages/nice/blocks/compressors.py:201: UserWarning: Amount of provided data is less than the desired one to fit PCA. Number of components is 200, desired number of environments is 2000, actual number of environments is 1000.\n",
      "  self.n_components, num_fit_now, X.shape[0]))\n"
     ]
    }
   ],
   "source": [
    "for key in train_coefficients.keys():\n",
    "    nice[key].fit(train_coefficients[key][:environments_for_fitting])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to fit different nice transformers for each central specie, see for example qm9 examples in example folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate representations!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {}\n",
    "for specie in all_species:\n",
    "    train_features[specie] = nice[specie].transform(train_coefficients[specie],\n",
    "                                                            return_only_invariants = True)\n",
    "    \n",
    "test_features = {}\n",
    "for specie in all_species:\n",
    "    test_features[specie] = nice[specie].transform(test_coefficients[specie],\n",
    "                                                           return_only_invariants = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is nested dictionary. First level keys are central species, inner level keys are body orders. Inside are numpy arrays with shapes [environment_index, invariant_index]:\n",
    "\n",
    "In this case number of train structures is 10k, and each consist of 4 H atoms. Thus, total amount of H centered environments is 40k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : (40000, 10)\n",
      "2 : (40000, 200)\n",
      "3 : (40000, 200)\n",
      "4 : (40000, 200)\n"
     ]
    }
   ],
   "source": [
    "for key in train_features[1].keys():\n",
    "    print(\"{} : {}\".format(key, train_features[1][key].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare to subsequent linear regression. As it was already discussed in theory energy is extensive property, and thus it is given as sum of atomic contributions. \n",
    "Each atomic contribution depends on 1) central specie and 2) environment. Thus, it is straighforward to see that if each atomic contribution is given by linear combination of previously calculated NICE features, the structural features should have the following form - for each structure set of features is a concatenation of representations for each specie. Representation for each specie is sum of NICE representations over the atoms with this specie in structure. \n",
    "\n",
    "In our case representation of each environment has size 200 + 200 + 200 + 10 = 610. And we have two atomic species - H and C. Thus shape of structures features should be [number_of_structures, 610 * 2 = 1220]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 8516.67it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8821.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1220)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_features = make_structural_features(train_features, train_structures, all_species)\n",
    "test_features = make_structural_features(test_features, test_structures, all_species)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energies are a part of the dataset we previously downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_energies = [structure.info['energy'] for structure in train_structures]\n",
    "train_energies = np.array(train_energies) * HARTREE_TO_EV\n",
    "\n",
    "test_energies = [structure.info['energy'] for structure in test_structures]\n",
    "test_energies = np.array(test_energies) * HARTREE_TO_EV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last step is to do linear regression and plot learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(first, second):\n",
    "    return np.sqrt(np.mean((first - second) ** 2))\n",
    "\n",
    "def get_standard_deviation(values):\n",
    "    return np.sqrt(np.mean((values - np.mean(values)) ** 2))\n",
    "\n",
    "def get_relative_performance(predictions, values):\n",
    "    return get_rmse(predictions, values) / get_standard_deviation(values)\n",
    "\n",
    "def estimate_performance(clf, data_train, data_test, targets_train, targets_test):\n",
    "    clf.fit(data_train, targets_train)\n",
    "    return get_relative_performance(clf.predict(data_test), targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:45<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for el in tqdm.tqdm(grid):   \n",
    "    errors.append(estimate_performance(BayesianRidge(), train_features[:el],\n",
    "                                       test_features, train_energies[:el],\n",
    "                                       test_energies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this smallest setup best rmse appeared to be about 7%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4246769917146664, 0.42795557159896896, 0.34949830448562647, 0.23167750292004954, 0.1805004317933902, 0.16245873101544683, 0.13090501832712487, 0.1162899615967867, 0.09911964945838202, 0.08351690671342994, 0.07210771368850206, 0.06984120801871395]\n"
     ]
    }
   ],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learnig curve looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(grid, errors, 'bo')\n",
    "plt.plot(grid, errors, 'b')\n",
    "plt.xlabel(\"number of structures\")\n",
    "plt.ylabel(\"relative error\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline in this tutorial was designed to expose all intermediate steps, but it has one drawback - at some moment all atomic representations along with all intermediate covariants for whole dataset are explicitly stored in RAM, which might become a bottleneck for big calculations. Indeed, only structural invariant features are eventually needed, and their size is much smaller than the size of all atomic representations, especially if dataset consist of large molecules. Thus, it is a good idea to calculate structural features by small blocks and get rid of atomic representations for each block immediatelly. For this purpose there is function nice.utilities.transform_sequentially. It has block_size parameter which controls the size of each chunk. The higher is this chunk the more RAM is required for calculations. But, on the other side for very small chunks slow python loops over lambda channels with invoking separate python classes for each lambda channel might become a bottleneck (all the other indices are handled either by numpy vectorization either by cython loops). Thus, transforming time per single environments monotonically decrease with the block_size and eventually goes to saturation. Default value for block_size parameter should be fine in most cases.\n",
    "\n",
    "Full example can be found in examples/methane_home_pc or in examples/qm9_home_pc. Other than that and absence of markdown comments these notebooks are almost identical to this tutorial (in qm9 single nice transformer is used for all central species). Thus, we recommend to pick one of them as the code snippet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
